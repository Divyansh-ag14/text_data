{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZWWH8Gvgd09_"
      },
      "source": [
        "# Word Embeddings in Action - Word2Vec\n",
        "\n",
        "Steps to follow:\n",
        "\n",
        "1. Get data\n",
        "2. Clean text data\n",
        "3. Tokenization\n",
        "4. Prepare vocabulary\n",
        "5. Download pre-trained embeddings\n",
        "6. Get word vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "gPnpxm2s7Zty"
      },
      "outputs": [],
      "source": [
        "# import required libraries\n",
        "import numpy as np\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P7ubotsYxoTi"
      },
      "source": [
        "# 1. Get Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "AgII8L1Xd0-Q"
      },
      "outputs": [],
      "source": [
        "#input text\n",
        "text=['Building some bots for Wikipedia.',\n",
        "      'Wikipedia is flooded with information.',\n",
        "      'There is an app for everthing.']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ndLOg1M3xw5u"
      },
      "source": [
        "# 2. Text Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "2DpVJTY4uzCM"
      },
      "outputs": [],
      "source": [
        "# cleaning\n",
        "import re\n",
        "\n",
        "def clean(text):\n",
        "  #lower case\n",
        "  text=text.lower()\n",
        "  \n",
        "  #remove punctuations\n",
        "  text=re.sub('[^a-zA-Z]',\" \",text)\n",
        "  \n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "9W8UmlIZu3Bq"
      },
      "outputs": [],
      "source": [
        "#call the clean function\n",
        "cleaned_text=[]\n",
        "\n",
        "for i in text:\n",
        "  cleaned_text.append(clean(i))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NSslu0ObyCxo"
      },
      "source": [
        "# 3. Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "colab_type": "code",
        "id": "HF8vhjlDu4Y-",
        "outputId": "56841446-2e6e-4b22-c6ea-e4dd18f67628"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['building', 'some', 'bots', 'for', 'wikipedia'], ['wikipedia', 'is', 'flooded', 'with', 'information'], ['there', 'is', 'an', 'app', 'for', 'everthing']]\n"
          ]
        }
      ],
      "source": [
        "#tokenize the text\n",
        "tokens=[]\n",
        "\n",
        "for i in cleaned_text:\n",
        "  tokens.append(i.split())\n",
        "\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OBcFBdRFyGzQ"
      },
      "source": [
        "# 4. Vocabulary Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "UNd7ic_WvFqO",
        "outputId": "e451bf84-7c66-4b2f-802b-09c9e023dd58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['an', 'app', 'bots', 'building', 'everthing', 'flooded', 'for', 'information', 'is', 'some', 'there', 'wikipedia', 'with']\n"
          ]
        }
      ],
      "source": [
        "#construct vocabulary\n",
        "vocab=[]\n",
        "\n",
        "for i in tokens:\n",
        "  for j in i:\n",
        "    if j not in vocab:\n",
        "      vocab.append(j)\n",
        "\n",
        "#remove duplicate token\n",
        "vocab = list(set(vocab))\n",
        "\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RMc9TTmSyO67"
      },
      "source": [
        "#5. Feature Representation (word2vec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5Fpt6-ned1AI"
      },
      "source": [
        "### Download Google's pre-trained Word2Vec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "colab_type": "code",
        "id": "WQNZ7Exh3jky",
        "outputId": "171824bb-d587-4c62-804f-fcb868906344"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2020-03-17 09:04:36--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.205.45\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.205.45|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  45.8MB/s    in 33s     \n",
            "\n",
            "2020-03-17 09:05:09 (47.2 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# download and extract word2vec embeddings \n",
        "! wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "! gunzip GoogleNews-vectors-negative300.bin.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "GoHGUSuId1AO"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# path of the downloaded model\n",
        "filename = 'GoogleNews-vectors-negative300.bin'\n",
        "\n",
        "# load into gensim\n",
        "w2vec = KeyedVectors.load_word2vec_format(filename, binary=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-5WyFsVHd1Aj"
      },
      "source": [
        "Once you have executed the above code, your word2vec embeddings are finally installed and loaded. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "assnsO6Ld1Ap"
      },
      "source": [
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "Please note that the length of every vector of the pre-trained word2vec embeddings is 300.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "xV0yy-P0sPKf"
      },
      "outputs": [],
      "source": [
        "# empty array of shape (no. of tokens X 300) to store word2vec features\n",
        "wordvec_array = np.zeros((len(vocab), 300))\n",
        "\n",
        "for i,j in enumerate(vocab):\n",
        "  wordvec_array[i,:] = w2vec.wv.word_vec(j)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "colab_type": "code",
        "id": "WpsWAoaap5AA",
        "outputId": "d8a0dab2-8b49-4941-c3e2-238f40c8cdb2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0.12597656,  0.19042969,  0.06982422, ...,  0.0612793 ,\n",
              "         0.17285156, -0.07861328],\n",
              "       [ 0.11572266, -0.29101562, -0.30664062, ..., -0.24609375,\n",
              "        -0.17773438,  0.16113281],\n",
              "       [ 0.41992188,  0.12011719, -0.06787109, ...,  0.00836182,\n",
              "        -0.25976562,  0.0279541 ],\n",
              "       ...,\n",
              "       [ 0.09423828, -0.02282715,  0.05224609, ..., -0.046875  ,\n",
              "         0.16113281, -0.19921875],\n",
              "       [ 0.21875   , -0.12207031, -0.00296021, ..., -0.35351562,\n",
              "        -0.25195312, -0.11621094],\n",
              "       [-0.02490234,  0.02197266, -0.03540039, ...,  0.01080322,\n",
              "        -0.01879883, -0.06884766]])"
            ]
          },
          "execution_count": 29,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wordvec_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "JRbv0MVl_hG1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Word_Embeddings_in_Action_Word2Vec.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.13 ('tensor')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "11cea9024f0d377440eaba48e9d7655bc3e56224c5f426395ac9dffd0677d0c7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
